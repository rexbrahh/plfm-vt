## ADR 0005: Open questions and recommendations

### 1) Ordering model: global sequence vs per-aggregate sequence

**Recommendation (v1): use both.**

* **Global monotonic event id** for replay and streaming:

  * `event_id BIGINT` generated by Postgres (`BIGSERIAL` or `GENERATED ALWAYS AS IDENTITY`)
* **Per-aggregate sequence** for invariants and conflict detection:

  * `aggregate_type`, `aggregate_id`, `aggregate_seq INT`

**Rules to enforce:**

* Unique constraint: `(aggregate_type, aggregate_id, aggregate_seq)` so you cannot create two “next events” for the same aggregate.
* For each command:

  * read current aggregate version (from a view or a lightweight aggregate checkpoint table)
  * append events with `aggregate_seq = prev + 1..n` inside one transaction

**Why this is the right shape:**

* Global `event_id` makes streaming simple and deterministic.
* Per-aggregate sequencing keeps your domain invariants honest (deploy state machines, volume attach rules, etc).

---

### 2) Which endpoints require read-your-writes semantics

**Recommendation (v1): explicitly classify endpoints into two buckets and implement “wait for projection” for bucket B.**

* **Bucket A (eventually consistent OK):**

  * list endpoints, dashboards, analytics, general reads
* **Bucket B (read-your-writes required):**

  * `deploy` returning the created release and its initial status
  * `create route` returning the route object
  * `set secrets` returning the new secret version metadata
  * anything that immediately returns an object the client expects to exist

**Implementation recommendation:**

* Every projector maintains a checkpoint: `projection_name -> last_applied_event_id`
* On Bucket B endpoints:

  1. append events, capture `new_event_id_max` in the same transaction
  2. block until required projections have `last_applied_event_id >= new_event_id_max` (with a short timeout)
  3. then read from the materialized view and return

This keeps the event log architecture intact while giving a good UX where it matters.

---

### 3) Exposing the change stream to agents and edge components

**Recommendation (v1): “streaming with polling fallback”, using event_id cursors.**

**Contract:**

* Every consumer maintains `cursor_event_id`.
* Consumers ask: “give me events after cursor, up to limit”.

**Transport:**

* Primary: **gRPC server-streaming** (or WebSocket) from control plane to agents/edge.
* Fallback: **polling** endpoint `GET /events?after=<id>&limit=<n>` for reconnects and simple clients.

**Wake-up mechanism (recommended):**

* Use Postgres `LISTEN/NOTIFY` (or an internal in-process pubsub) to wake stream loops.
* Always fetch actual events from Postgres by `event_id` range, so no one can miss events due to notify loss.

**Filtering recommendation:**

* Do not let agents subscribe to “everything”.
* Provide server-side filters by:

  * `node_id` for scheduling decisions and allocations
  * “edge relevant” event types for routing updates
  * “secrets relevant” only for the node that will run the env

This stays simple, durable, and debuggable, and it scales until you genuinely need a dedicated event bus.

## ADR 0006: Open questions and recommendations

### 1) Event ordering model and indexing strategy

**Recommendation (v1): one append-only `events` table with a global monotonic id, plus the per-aggregate sequencing model from ADR 0005.**

Concrete schema shape (minimum):

* `event_id BIGINT GENERATED ALWAYS AS IDENTITY` (primary key)
* `occurred_at TIMESTAMPTZ NOT NULL DEFAULT now()`
* `aggregate_type TEXT NOT NULL`
* `aggregate_id UUID NOT NULL` (or ULID, but pick one and stick to it)
* `aggregate_seq INT NOT NULL`
* `event_type TEXT NOT NULL`
* `actor_id UUID` (or service principal id)
* `payload JSONB NOT NULL`

Indexes (v1):

* PK on `event_id`
* UNIQUE `(aggregate_type, aggregate_id, aggregate_seq)`
* INDEX `(aggregate_type, aggregate_id, aggregate_seq DESC)` for fast “latest version”
* INDEX `(event_type, event_id)` for consumers that filter by type
* Optional INDEX on `(occurred_at)` for audit queries

Partitioning:

* **Do not partition in v1.** Only add time or id range partitioning after you have measured real volume.

Immutability enforcement:

* revoke `UPDATE/DELETE` on `events` for the app role
* optional trigger that raises on update/delete as a second guardrail

---

### 2) Replication and failover posture for early production

**Recommendation (v1): single primary Postgres with one warm standby via streaming replication, manual failover with a runbook.**

Why:

* keeps operational complexity low while avoiding “one disk dies and you are dead”
* you can evolve to automated failover once you have stable ops signals

Concrete posture:

* Primary on a dedicated machine (or dedicated VM) in your primary region
* Standby in a second machine (same region is fine for v1; different region later)
* Replication: asynchronous streaming replication
* Promotion: manual `pg_ctl promote` (or Patroni later)

If you strongly prefer “outsourced” ops:

* move to managed Postgres later, but do not block v1 on it. Your state model already supports migration if you plan it.

---

### 3) Backup cadence, retention, and restore testing

**Recommendation (v1): WAL archiving + periodic full backups, with regular restore drills.**

Concrete plan:

* WAL archiving continuously (gives you point-in-time restore)
* Full backup daily (or weekly full + daily differential if you want to reduce bandwidth)
* Retention: 14 to 30 days (pick one, document it)
* Encryption: encrypt backups at rest (and in transit)

Restore drills (non-negotiable):

* Automated restore test at least weekly:

  * restore into a scratch Postgres instance
  * run a basic integrity check (can the control plane start, do projections rebuild, does the demo script pass)
* Track: time to restore, last successful restore date

Targets to write down (v1 realistic):

* RPO: “up to the last archived WAL segment”
* RTO: “restore + restart control plane within a documented window” (set an initial goal like under 60 minutes, then tighten later based on drills)

## ADR 0007: Open questions and recommendations

### 1) Unit of allocation for IPv4 add-on: per environment vs per app vs per hostname

**Recommendation (v1): allocate IPv4 per environment.**

Meaning:

* `(org, app, env)` gets one dedicated public IPv4 when the add-on is enabled.
* All public routes for that env can bind to that IPv4 subject to port policy.

Why:

* Matches your other scoping decisions (secrets, config, isolation are env-scoped).
* Keeps billing and ownership unambiguous.
* Simplifies operations: “this env owns this IPv4” is easy to reason about.

What we do not do in v1:

* per-hostname IPv4 (too many edge cases, and it encourages IP sprawl)
* per-app IPv4 (blurs staging/prod boundaries and makes incidents messier)

---

### 2) Pricing model: fixed port bundle vs per-port pricing

**Recommendation (v1): fixed monthly IPv4 fee + included small port bundle, then per-extra-port.**

Concrete:

* Base IPv4 add-on fee covers:

  * 1 dedicated IPv4 address
  * a default set of ports (example: 80/443 plus a small number of arbitrary TCP ports, like 2 to 5)
* Additional public TCP ports beyond the bundle incur an extra fee.

Why:

* Predictable for users.
* Reflects the real scarcity cost of the IPv4 address.
* Stops abuse where someone binds hundreds of ports “because it’s free”.

If you want to keep v1 simpler:

* Do only “IPv4 address fee includes up to N ports” and don’t introduce per-port billing until later.

---

### 3) Do we introduce a shared IPv4 tier later

**Recommendation: keep it out of v1, but design the model so it can exist.**

Spec decision:

* Shared IPv4 tier could be:

  * HTTP/HTTPS only (L7 termination) or
  * a limited port-sharing model with strict constraints

But it should not be your default story, because it conflicts with “L4 passthrough” and creates noisy-neighbor risk.

Implementation guardrail now:

* Build the route model so it can target either:

  * `endpoint = { ipv6_only }`
  * `endpoint = { dedicated_ipv4 }`
  * later: `endpoint = { shared_ipv4_pool }`
    without changing the API surface.

---

### Extra operational recommendation (should be explicit)

**Ingress should remain IPv6-native internally even when serving IPv4 clients.**
For IPv4-enabled envs, terminate IPv4 at the edge and forward to backends over IPv6 (overlay), so you do not pollute internal systems with IPv4 assumptions.

## ADR 0008: Open questions and recommendations

### 1) Default ports and constraints: what is allowed on IPv6-only vs with IPv4 add-on

**Recommendation (v1):**

* **IPv6-only default:** allow **80 and 443** (plus optionally a small set of additional TCP ports behind an explicit allowlist).
* **With dedicated IPv4 add-on:** allow **80 and 443** plus **explicit user-requested TCP ports** (raw TCP) subject to a platform allowlist/denylist.

**Concrete policy to write down:**

* Default public exposure is:

  * `tcp/443` (recommended)
  * `tcp/80` optional (redirect is user’s responsibility unless you later add an L7 feature)
* Additional ports must be explicitly declared in the manifest route config and approved by policy.
* Denylist ports that are operationally risky or commonly abused (SMTP 25 as a classic example) unless you intentionally want to support them.

Why:

* Keeps “random port exposure” from turning into an abuse magnet.
* Matches your “raw TCP is first-class but explicit”.

---

### 2) How we model “hostnames” in the control plane

**Recommendation (v1): introduce a first-class `Route` object, environment-scoped, with explicit hostname and port mapping.**

Minimal shape:

* `Route` belongs to `(org, app, env)`
* fields:

  * `hostname` (FQDN)
  * `listen_port` (usually 443, or an explicit TCP port)
  * `protocol_hint` (`tls_passthrough`, `tcp_raw`, later `http_terminate`)
  * `backend_process_type` (usually `web`)
  * `backend_port` (port inside the microVM)
  * `proxy_protocol` on/off (ADR 0009)
  * `ipv4_required` boolean (drives add-on requirement)
  * `health_source` (which process/port determines route health)

**Ownership rule:**

* A hostname can only be bound to one environment at a time within the platform.
* Changing a hostname binding is a state transition with audit events.

Why:

* Prevents implicit behavior and makes routing debuggable.
* Keeps it compatible with SNI routing and future L7 opt-in.

---

### 3) Strategy for clients without SNI

**Recommendation (v1):**

* **Default stance:** require SNI for hostname-based TLS routes.
* If SNI is absent:

  * only support via **dedicated IP-based routing** (typically requires the IPv4 add-on, or a dedicated IPv6 address if you choose to productize that later).

**Concrete behavior:**

* For a TLS connection without SNI:

  * if the listener has exactly one route bound to that `(IP,port)` pair, you may route to it
  * otherwise reject with a connection close (no guessing)

Why:

* Non-SNI TLS is rare for modern clients, but ambiguity is dangerous.
* This preserves deterministic routing without building a complicated “default backend” that becomes a security and incident risk.

---

### Extra recommendation (clarifies “L4 does not terminate”)

**Define “SNI passthrough” as:**

* We may **inspect** the ClientHello to read SNI.
* We do **not terminate TLS**, do not present certs, do not modify payload bytes other than optional PROXY v2 prefix when enabled on the route.

This should be spelled out so nobody accidentally implements “TLS termination” under the label “SNI routing.”
